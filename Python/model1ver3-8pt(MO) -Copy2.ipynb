{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9f3d2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Data Science snippet\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm_notebook\n",
    "from scipy.spatial.distance import cdist\n",
    "import imageio\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.collections import PatchCollection\n",
    "\n",
    "plt.style.use(\"seaborn-dark\")\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from rl.agents.q_agent import QAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74ea2af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "    \n",
    "    def __init__(self, n_stops=8,method=\"angle_difference\", **kwargs):\n",
    "        \n",
    "        print(f\"Initialized Delivery Environment with {n_stops} random stops\")\n",
    "        print(f\"Target metric for optimization is {method}\")\n",
    "              \n",
    "        #Initializaiton \n",
    "        self.n_stops = n_stops\n",
    "        self.action_space = self.n_stops\n",
    "        self.observation_space = self.n_stops\n",
    "        self.stops = []\n",
    "        self.method = method\n",
    "        \n",
    "        \n",
    "        #Generate Stops \n",
    "        self._generate_stops()\n",
    "        self._generate_q_values()\n",
    "        self.render\n",
    "        self.reset()\n",
    "        \n",
    "        \n",
    "    def _generate_stops(self):\n",
    "        \n",
    "        xy = np.loadtxt('test.csv', delimiter=\",\")\n",
    "        self.x = xy[:,0]\n",
    "        self.y = xy[:,1]\n",
    "    \n",
    "        \n",
    "    def render(self,return_img = False):\n",
    "        \n",
    "        fig = plt.figure(figsize=(7,7))\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.set_title(\"Stops\")\n",
    "\n",
    "        # Show stops\n",
    "        ax.scatter(self.x,self.y,c = \"red\",s = 50)\n",
    "\n",
    "        # Show START\n",
    "        if len(self.stops)>0:\n",
    "            xy = self._get_xy(initial = True)\n",
    "            xytext = xy[0]+0.1,xy[1]-0.05\n",
    "            ax.annotate(\"START\",xy=xy,xytext=xytext,weight = \"bold\")\n",
    "\n",
    "        # Show itinerary\n",
    "        if len(self.stops) > 1:\n",
    "            ax.plot(self.x[self.stops],self.y[self.stops],c = \"blue\",linewidth=1,linestyle=\"--\")\n",
    "            \n",
    "            # Annotate END\n",
    "            xy = self._get_xy(initial = False)\n",
    "            xytext = xy[0]+0.1,xy[1]-0.05\n",
    "            ax.annotate(\"END\",xy=xy,xytext=xytext,weight = \"bold\")\n",
    "\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        \n",
    "        if return_img:\n",
    "            # From https://ndres.me/post/matplotlib-animated-gifs-easily/\n",
    "            fig.canvas.draw_idle()\n",
    "            image = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n",
    "            image  = image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "            plt.close()\n",
    "            return image\n",
    "        else:\n",
    "            plt.show()\n",
    "    \n",
    "    def reset(self):\n",
    "\n",
    "        # Stops placeholder\n",
    "        self.stops = [7]\n",
    "\n",
    "        # Random first stop\n",
    "        return self.stops[-1]\n",
    "\n",
    "    def step(self,destination):\n",
    "\n",
    "        # Get current state\n",
    "        state = self._get_state()\n",
    "        new_state = destination\n",
    "        \n",
    "        # Get reward for such a move\n",
    "        reward = self._get_reward(state,new_state)\n",
    "\n",
    "        # Append new_state to stops\n",
    "        self.stops.append(destination)\n",
    "        done = len(self.stops) == self.n_stops\n",
    "\n",
    "        return new_state,reward,done\n",
    "\n",
    "    def _get_state(self):\n",
    "        return self.stops[-1]\n",
    "\n",
    "\n",
    "    def _get_xy(self,initial = False):\n",
    "        state = self.stops[0] if initial else self._get_state()\n",
    "        x = self.x[state]\n",
    "        y = self.y[state]\n",
    "        return x,y\n",
    "\n",
    "    def _get_reward(self,state,new_state):\n",
    "        current_point = (self.x[state], self.y[state])\n",
    "        next_point = (self.env.x[next_state], self.env.y[next_state])\n",
    "\n",
    "        # Find the previous point\n",
    "        prev_point_index = state - 1 if state > 0 else self.env.n_stops - 1\n",
    "        prev_point = (self.env.x[prev_point_index], self.env.y[prev_point_index])\n",
    "\n",
    "        # Calculate the angle of incidence\n",
    "        angle_of_incidence = angle_between_points(prev_point, current_point)\n",
    "\n",
    "        # Calculate the angle of reflection\n",
    "        angle_of_reflection = angle_between_points(current_point, next_point)\n",
    "\n",
    "        # Calculate the difference between angle of incidence and angle of reflection\n",
    "        angle_difference = abs(angle_of_reflection - angle_of_incidence)\n",
    "\n",
    "        # Return the negative difference as the reward (minimizing the difference)\n",
    "        return -angle_difference\n",
    "\n",
    "def run_episode(env,agent,verbose = 1):\n",
    "\n",
    "    s = env.reset()\n",
    "    agent.reset_memory()\n",
    "    max_step = env.n_stops \n",
    "\n",
    "    episode_reward = 0\n",
    "    \n",
    "    # Remember the states\n",
    "    agent.remember_state(s)\n",
    "    # Take the action, and get the reward from environment\n",
    "    s_next,r,done = env.step(0)\n",
    "\n",
    "    # Tweak the reward\n",
    "    r = r\n",
    "    if verbose: print(s_next,r,done)\n",
    "\n",
    "    # Update our knowledge ein the Q-table\n",
    "    agent.train(s,0,r,s_next)\n",
    "\n",
    "    # Update the caches\n",
    "    episode_reward += r\n",
    "    s = s_next\n",
    "\n",
    "    i = 0\n",
    "    while i < max_step-1:\n",
    "        \n",
    "        # Remember the states\n",
    "        agent.remember_state(s)\n",
    "        # Choose an action\n",
    "        a = agent.act(s)\n",
    "        # Take the action, and get the reward from environment\n",
    "        s_next,r,done = env.step(a)\n",
    "\n",
    "        # Tweak the reward\n",
    "        r = r\n",
    "\n",
    "        if verbose: print(s_next,r,done)\n",
    "\n",
    "        # Update our knowledge ein the Q-table\n",
    "        agent.train(s,a,r,s_next)\n",
    "\n",
    "        # Update the caches\n",
    "        episode_reward += r\n",
    "        s = s_next\n",
    "\n",
    "        # If the episode is terminated\n",
    "        i += 1\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    return env,agent,episode_reward\n",
    "\n",
    "\n",
    "class DeliveryQAgent(QAgent):\n",
    "\n",
    "    def __init__(self, env, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.env = env\n",
    "        self.reset_memory()\n",
    "\n",
    "    def act(self, s):\n",
    "        # Get Q Vector\n",
    "        q = np.copy(self.Q[s, :])\n",
    "\n",
    "        # Avoid already visited states\n",
    "        q[self.states_memory] = -np.inf\n",
    "\n",
    "        # Filter out invalid actions (path from non-zero y to non-zero y and from x-axis to x-axis)\n",
    "        valid_actions = [a for a in range(self.actions_size) if (self.env.y[a] == 0 and self.env.y[s] != 0) or (self.env.y[a] != 0 and self.env.y[s] == 0)]\n",
    "\n",
    "        if np.random.rand() > self.epsilon:\n",
    "            # Choose the action with the highest Q-value among valid actions\n",
    "            valid_q = q[valid_actions]\n",
    "            a = valid_actions[np.argmax(valid_q)]\n",
    "        else:\n",
    "            # Choose a random valid action\n",
    "            a = np.random.choice(valid_actions)\n",
    "\n",
    "        return a\n",
    "\n",
    "\n",
    "    def remember_state(self,s):\n",
    "        self.states_memory.append(s)\n",
    "\n",
    "    def reset_memory(self):\n",
    "        self.states_memory = []\n",
    "\n",
    "    \n",
    "def run_n_episodes(env,agent,name=\"training.gif\", name2 =\"optimal8.gif\",n_episodes=1000,render_each=10,fps=10):\n",
    "\n",
    "    # Store the rewards\n",
    "    rewards = []\n",
    "    imgs = []\n",
    "\n",
    "    # Experience replay\n",
    "    for i in tqdm_notebook(range(n_episodes)):\n",
    "\n",
    "        # Run the episode\n",
    "        env,agent,episode_reward = run_episode(env,agent,verbose = 0)\n",
    "        rewards.append(episode_reward)\n",
    "        \n",
    "        if i % render_each == 0:\n",
    "            img = env.render(return_img = True)\n",
    "            imgs.append(img)\n",
    "\n",
    "    \n",
    "    # Show rewards\n",
    "    plt.figure(figsize = (15,3))\n",
    "    plt.title(\"Rewards over training\")\n",
    "    plt.plot(rewards)\n",
    "    plt.show()\n",
    "\n",
    "    # Save imgs as gif\n",
    "    imageio.mimsave(name,imgs,fps = fps)\n",
    "    return env,agent, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a70a9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_corr_episode(env,agent,verbose = 1):\n",
    "\n",
    "    s = env.reset()\n",
    "    agent.reset_memory()\n",
    "    arr = [0, 1, 2, 3, 4, 5, 6]\n",
    "\n",
    "    episode_reward = 0\n",
    "\n",
    "    for i in arr:\n",
    "                  \n",
    "        # Remember the states\n",
    "        agent.remember_state(s)\n",
    "        \n",
    "        # Take the action, and get the reward from environment\n",
    "        s_next,r,done = env.step(i)\n",
    "\n",
    "        # Tweak the reward\n",
    "        r = r\n",
    "\n",
    "        if verbose: print(s_next,r,done)\n",
    "\n",
    "        # Update our knowledge ein the Q-table\n",
    "        agent.train(s,i,r,s_next)\n",
    "\n",
    "        # Update the caches\n",
    "        episode_reward += r\n",
    "        s = s_next\n",
    "\n",
    "        # If the episode is terminated\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    return env,agent,episode_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26eea55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Delivery Environment with 8 random stops\n",
      "Target metric for optimization is angle_difference\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Environment' object has no attribute 'env'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16776/3326981845.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEnvironment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_stops\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"angle_difference\"\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDeliveryQAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_corr_episode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Iteration Reward = \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16776/61904878.py\u001b[0m in \u001b[0;36mrun_corr_episode\u001b[1;34m(env, agent, verbose)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;31m# Take the action, and get the reward from environment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0ms_next\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m# Tweak the reward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16776/728007582.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, destination)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[1;31m# Get reward for such a move\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m         \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_reward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[1;31m# Append new_state to stops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16776/728007582.py\u001b[0m in \u001b[0;36m_get_reward\u001b[1;34m(self, state, new_state)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_reward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mcurrent_point\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m         \u001b[0mnext_point\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Environment' object has no attribute 'env'"
     ]
    }
   ],
   "source": [
    "#Correct Reward Path For Light\n",
    "env = Environment(n_stops = 8,method = \"angle_difference\" )\n",
    "agent = DeliveryQAgent(env,env.observation_space,env.action_space)\n",
    "env, agent, reward = run_corr_episode(env,agent)\n",
    "print(\"Iteration Reward = \", reward)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3b8ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_hyperparameters(env, hyperparameter_grid, n_episodes=1000, render_each=10, fps=10):\n",
    "    best_hyperparameters = None\n",
    "    best_mean_reward = float(\"-inf\")\n",
    "    rewards_by_hyperparameters = {}\n",
    "\n",
    "    # Loop through all possible combinations of hyperparameters\n",
    "    for hyperparameters in hyperparameter_grid:\n",
    "        print(f\"Training with hyperparameters: {hyperparameters}\")\n",
    "\n",
    "        # Create the agent with the current set of hyperparameters\n",
    "        agent = DeliveryQAgent(env,states_size=env.observation_space, actions_size=env.action_space, \n",
    "                               epsilon=hyperparameters['epsilon'],\n",
    "                               epsilon_decay=hyperparameters['epsilon_decay'],\n",
    "                               gamma=hyperparameters['gamma'],\n",
    "                               lr=hyperparameters['lr'])\n",
    "\n",
    "        # Train the agent and get rewards\n",
    "        _, _, rewards = run_n_episodes(env, agent, n_episodes=n_episodes, render_each=render_each, fps=fps)\n",
    "        rewards_by_hyperparameters[str(hyperparameters)] = rewards\n",
    "\n",
    "        # Calculate the mean reward over the last episodes\n",
    "        mean_reward = np.mean(rewards[-100:])\n",
    "        print(f\"Mean reward over the last 100 episodes: {mean_reward}\")\n",
    "\n",
    "        # Check if it's the best set of hyperparameters so far\n",
    "        if mean_reward > best_mean_reward:\n",
    "            best_mean_reward = mean_reward\n",
    "            best_hyperparameters = hyperparameters\n",
    "\n",
    "    print(\"Grid search complete.\")\n",
    "    print(\"Best Hyperparameters:\")\n",
    "    print(best_hyperparameters)\n",
    "\n",
    "    return best_hyperparameters, rewards_by_hyperparameters\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the hyperparameter grid with the range of values to search through\n",
    "    hyperparameter_grid = [\n",
    "        {\"epsilon\": 0.2, \"epsilon_decay\": 0.8, \"gamma\": 0.7, \"lr\": 0.1},\n",
    "        {\"epsilon\": 0.4, \"epsilon_decay\": 0.9, \"gamma\": 0.8, \"lr\": 0.3},\n",
    "        {\"epsilon\": 0.6, \"epsilon_decay\": 0.95, \"gamma\": 0.9, \"lr\": 0.5},\n",
    "        {\"epsilon\": 0.8, \"epsilon_decay\": 0.95, \"gamma\": 0.9, \"lr\": 0.7}\n",
    "    ]\n",
    "\n",
    "    # Create the environment\n",
    "    env = Environment(n_stops=8)\n",
    "\n",
    "    # Perform grid search hyperparameter tuning\n",
    "    best_hyperparameters, rewards_by_hyperparameters = grid_search_hyperparameters(env, hyperparameter_grid)\n",
    "\n",
    "    # Train the final agent with the best hyperparameters\n",
    "    agent = DeliveryQAgent(env,states_size=env.observation_space, actions_size=env.action_space, \n",
    "                           epsilon=best_hyperparameters['epsilon'],\n",
    "                           epsilon_decay=best_hyperparameters['epsilon_decay'],\n",
    "                           gamma=best_hyperparameters['gamma'],\n",
    "                           lr=best_hyperparameters['lr'])\n",
    "    p, q, r = run_n_episodes(env, agent, n_episodes=1000, render_each=10, fps=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b577c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = r[-100:]==reward\n",
    "count_true = np.count_nonzero(result)\n",
    "print(\"Iterations matching optimal reward = \", count_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9ed1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename=\"training.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d4ca18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24657c92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
