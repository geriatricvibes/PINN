{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04585a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import math\n",
    "from collections import namedtuple\n",
    "import os\n",
    "import time\n",
    "\n",
    "from scipy.spatial import distance_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from scipy.signal import medfilt\n",
    "\n",
    "\"\"\" Note: the code is not optimized for GPU\n",
    "\"\"\"\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed1df61a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.16601049,  5.16601049],\n",
       "       [ 6.00383107,  0.        ],\n",
       "       [ 6.74624857,  4.57775407],\n",
       "       [ 3.35733921,  0.        ],\n",
       "       [-1.05853617,  5.96498441],\n",
       "       [-4.35714012,  0.        ],\n",
       "       [-6.86150897,  4.52874046],\n",
       "       [ 0.        ,  0.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Mathematica exported pt array to python array\n",
    "array = np.loadtxt('test.csv', delimiter=\",\")\n",
    "array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d070abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xaxis array = \n",
      " [[ 6.00383107  0.        ]\n",
      " [ 3.35733921  0.        ]\n",
      " [-4.35714012  0.        ]\n",
      " [ 0.          0.        ]]\n",
      "curve array = \n",
      " [[ 5.16601049  5.16601049]\n",
      " [ 6.74624857  4.57775407]\n",
      " [-1.05853617  5.96498441]\n",
      " [-6.86150897  4.52874046]]\n"
     ]
    }
   ],
   "source": [
    "#Separate x axis and curve pts \n",
    "def ptseparator(array):\n",
    "    xaxis = list(list())\n",
    "    curve = list(list())\n",
    "    for i in array:\n",
    "        if i[1]==0:\n",
    "            xaxis.append(i)\n",
    "        else:\n",
    "            curve.append(i)\n",
    "    return xaxis, curve  \n",
    "\n",
    "xaxis, curve = np.array(ptseparator(array))\n",
    "print(\"xaxis array = \\n\", xaxis)\n",
    "print(\"curve array = \\n\", curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20993225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting of the Points\n",
    "x, y = xaxis.T\n",
    "plt.scatter(x,y);\n",
    "\n",
    "x, y = curve.T\n",
    "plt.scatter(x,y);\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3e2d2a05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def slope_func(point, x):\n",
    "    if (point[1]-x[1]) ==0:\n",
    "        s = +100\n",
    "    elif (point[1]!=0) and (x[1]!=0):\n",
    "        s = +100\n",
    "    else: \n",
    "        s = (point[0]-x[0])/(point[1]-x[1])\n",
    "    return s\n",
    "\n",
    "\n",
    "def slope_matrix(a, b):\n",
    "    mat = np.zeros((len(a), len(b)))\n",
    "    for i in range(len(a)):\n",
    "        for j in range(len(b)):\n",
    "            mat[i][j] = slope_func(a[j], b[i]);\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "20eea6d1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[100.     -0.162 100.      0.35  100.      1.843 100.      1.   ]\n",
      " [ -0.162 100.      0.162 100.     -1.184 100.     -2.841 100.   ]\n",
      " [100.      0.162 100.      0.74  100.      2.426 100.      1.474]\n",
      " [  0.35  100.      0.74  100.     -0.74  100.     -2.256 100.   ]\n",
      " [100.     -1.184 100.     -0.74  100.      0.553 100.     -0.177]\n",
      " [  1.843 100.      2.426 100.      0.553 100.     -0.553 100.   ]\n",
      " [100.     -2.841 100.     -2.256 100.     -0.553 100.     -1.515]\n",
      " [  1.    100.      1.474 100.     -0.177 100.     -1.515 100.   ]]\n"
     ]
    }
   ],
   "source": [
    "#Slope Matrix ((i, j) -> i = pts o2f x axis , j = pts on curve, (i, j) represents slope from j to i \n",
    "np.set_printoptions(precision=3)\n",
    "sl_mat = slope_matrix(coords, coords)\n",
    "print(sl_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a0ba893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d61e7e2",
   "metadata": {},
   "source": [
    "## Define the state\n",
    "We now define the state tuple, containing a graph (given by a weights matrix `W`), the noode coordinates `coords` and the partial solution (list of visited nodes).\n",
    "We also define the function `state2tens`, which translates such tuples into tensors (partially loosing the sequence order information)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f9f87c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "State = namedtuple('State', ('W', 'coords', 'partial_solution'))\n",
    "  \n",
    "def state2tens(state):\n",
    "    \"\"\" Creates a Pytorch tensor representing the history of visited nodes, from a (single) state tuple.\n",
    "        \n",
    "        Returns a (Nx5) tensor, where for each node we store whether this node is in the sequence,\n",
    "        whether it is first or last, and its (x,y) coordinates.\n",
    "    \"\"\"\n",
    "    solution = set(state.partial_solution)\n",
    "    sol_last_node = state.partial_solution[-1] if len(state.partial_solution) > 0 else -1\n",
    "    sol_first_node = state.partial_solution[0] if len(state.partial_solution) > 0 else -1\n",
    "    coords = state.coords\n",
    "    nr_nodes = coords.shape[0]\n",
    "\n",
    "    xv = [[(1 if i in solution else 0),\n",
    "           (1 if i == sol_first_node else 0),\n",
    "           (1 if i == sol_last_node else 0),\n",
    "           coords[i,0],\n",
    "           coords[i,1]\n",
    "          ] for i in range(nr_nodes)]\n",
    "    \n",
    "    return torch.tensor(xv, dtype=torch.float32, requires_grad=False, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948b535d",
   "metadata": {},
   "source": [
    "## The Q-Function\n",
    "Below, we write the neural network that will parameterize the function Q(s, a).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c0bcfde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNet(nn.Module):\n",
    "    \"\"\" The neural net that will parameterize the function Q(s, a)\n",
    "    \n",
    "        The input is the state (containing the graph and visited nodes),\n",
    "        and the output is a vector of size N containing Q(s, a) for each of the N actions a.\n",
    "    \"\"\"    \n",
    "    \n",
    "    def __init__(self, emb_dim, T=4):\n",
    "        \"\"\" emb_dim: embedding dimension p\n",
    "            T: number of iterations for the graph embedding\n",
    "        \"\"\"\n",
    "        super(QNet, self).__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.T = T\n",
    "        \n",
    "        # We use 5 dimensions for representing the nodes' states:\n",
    "        # * A binary variable indicating whether the node has been visited\n",
    "        # * A binary variable indicating whether the node is the first of the visited sequence\n",
    "        # * A binary variable indicating whether the node is the last of the visited sequence\n",
    "        # * The (x, y) coordinates of the node.\n",
    "        self.node_dim = 5\n",
    "        \n",
    "        # We can have an extra layer after theta_1 (for the sake of example to make the network deeper)\n",
    "        nr_extra_layers_1 = 1\n",
    "        \n",
    "        # Build the learnable affine maps:\n",
    "        self.theta1 = nn.Linear(self.node_dim, self.emb_dim, True)\n",
    "        self.theta2 = nn.Linear(self.emb_dim, self.emb_dim, True)\n",
    "        self.theta3 = nn.Linear(self.emb_dim, self.emb_dim, True)\n",
    "        self.theta4 = nn.Linear(1, self.emb_dim, True)\n",
    "        self.theta5 = nn.Linear(2*self.emb_dim, 1, True)\n",
    "        self.theta6 = nn.Linear(self.emb_dim, self.emb_dim, True)\n",
    "        self.theta7 = nn.Linear(self.emb_dim, self.emb_dim, True)\n",
    "        \n",
    "        self.theta1_extras = [nn.Linear(self.emb_dim, self.emb_dim, True) for _ in range(nr_extra_layers_1)]\n",
    "        \n",
    "    def forward(self, xv, Ws):\n",
    "        # xv: The node features (batch_size, num_nodes, node_dim)\n",
    "        # Ws: The graphs (batch_size, num_nodes, num_nodes)\n",
    "        \n",
    "        num_nodes = xv.shape[1]\n",
    "        batch_size = xv.shape[0]\n",
    "        \n",
    "        # pre-compute 1-0 connection matrices masks (batch_size, num_nodes, num_nodes)\n",
    "        conn_matrices = torch.where(Ws > 0, torch.ones_like(Ws), torch.zeros_like(Ws)).to(device)\n",
    "        \n",
    "        # Graph embedding\n",
    "        # Note: we first compute s1 and s3 once, as they are not dependent on mu\n",
    "        mu = torch.zeros(batch_size, num_nodes, self.emb_dim, device=device)\n",
    "        s1 = self.theta1(xv)  # (batch_size, num_nodes, emb_dim)\n",
    "        for layer in self.theta1_extras:\n",
    "            s1 = layer(F.relu(s1))  # we apply the extra layer\n",
    "        \n",
    "        s3_1 = F.relu(self.theta4(Ws.unsqueeze(3)))  # (batch_size, nr_nodes, nr_nodes, emb_dim) - each \"weigth\" is a p-dim vector        \n",
    "        s3_2 = torch.sum(s3_1, dim=1)  # (batch_size, nr_nodes, emb_dim) - the embedding for each node\n",
    "        s3 = self.theta3(s3_2)  # (batch_size, nr_nodes, emb_dim)\n",
    "        \n",
    "        for t in range(self.T):\n",
    "            s2 = self.theta2(conn_matrices.matmul(mu))    \n",
    "            mu = F.relu(s1 + s2 + s3)\n",
    "            \n",
    "        \"\"\" prediction\n",
    "        \"\"\"\n",
    "        # we repeat the global state (summed over nodes) for each node, \n",
    "        # in order to concatenate it to local states later\n",
    "        global_state = self.theta6(torch.sum(mu, dim=1, keepdim=True).repeat(1, num_nodes, 1))\n",
    "        \n",
    "        local_action = self.theta7(mu)  # (batch_dim, nr_nodes, emb_dim)\n",
    "            \n",
    "        out = F.relu(torch.cat([global_state, local_action], dim=2))\n",
    "        return self.theta5(out).squeeze(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b1552218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model output: tensor([[64.6817, 64.6546, 65.7846, 64.6543, 64.6494, 64.6554, 65.8637, 64.6544]],\n",
      "       grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" See what the model returns\n",
    "\"\"\"\n",
    "model = QNet(3, T=1).to(device)\n",
    "coords, W_np = coords, sl_mat\n",
    "W = torch.tensor(W_np, dtype=torch.float32, device=device)\n",
    "xv = torch.rand((1, W.shape[0], 5)).to(device) # random node state\n",
    "Ws = W.unsqueeze(0)\n",
    "\n",
    "y = model(xv, Ws)\n",
    "print('model output: {}'.format(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a8745f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cb6dcf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QFunction():\n",
    "    def __init__(self, model, optimizer, lr_scheduler):\n",
    "        self.model = model  # The actual QNet\n",
    "        self.optimizer = optimizer\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "    \n",
    "    def predict(self, state_tsr, W):\n",
    "        # batch of 1 - only called at inference time\n",
    "        with torch.no_grad():\n",
    "            estimated_rewards = self.model(state_tsr.unsqueeze(0), W.unsqueeze(0))\n",
    "        return estimated_rewards[0]\n",
    "                \n",
    "    def get_best_action(self, state_tsr, state):\n",
    "        \"\"\" Computes the best (greedy) action to take from a given state\n",
    "            Returns a tuple containing the ID of the next node and the corresponding estimated reward\n",
    "        \"\"\"\n",
    "        W = state.W\n",
    "        estimated_rewards = self.predict(state_tsr, W)  # size (nr_nodes,)\n",
    "        sorted_reward_idx = estimated_rewards.argsort(descending=True)\n",
    "        \n",
    "        solution = state.partial_solution\n",
    "        \n",
    "        already_in = set(solution)\n",
    "        for idx in sorted_reward_idx.tolist():\n",
    "            if (len(solution) == 0 or W[solution[-1], idx] > 0) and idx not in already_in:\n",
    "                return idx, estimated_rewards[idx].item()\n",
    "        \n",
    "    def batch_update(self, states_tsrs, Ws, actions, targets):\n",
    "        \"\"\" Take a gradient step using the loss computed on a batch of (states, Ws, actions, targets)\n",
    "        \n",
    "            states_tsrs: list of (single) state tensors\n",
    "            Ws: list of W tensors\n",
    "            actions: list of actions taken\n",
    "            targets: list of targets (resulting estimated rewards after taking the actions)\n",
    "        \"\"\"        \n",
    "        Ws_tsr = torch.stack(Ws).to(device)\n",
    "        xv = torch.stack(states_tsrs).to(device)\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # the rewards estimated by Q for the given actions\n",
    "        estimated_rewards = self.model(xv, Ws_tsr)[range(len(actions)), actions]\n",
    "        \n",
    "        loss = self.loss_fn(estimated_rewards, torch.tensor(targets, device=device))\n",
    "        loss_val = loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        self.optimizer.step()        \n",
    "        self.lr_scheduler.step()\n",
    "        \n",
    "        return loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6ff03c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: we store state tensors in experience to compute these tensors only once later on\n",
    "Experience = namedtuple('Experience', ('state', 'state_tsr', 'action', 'reward', 'next_state', 'next_state_tsr'))\n",
    "\n",
    "class Memory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "        self.nr_inserts = 0\n",
    "        \n",
    "    def remember(self, experience):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = experience\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        self.nr_inserts += 1\n",
    "        \n",
    "    def sample_batch(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return min(self.nr_inserts, self.capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6e951559",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_distance(solution, W):\n",
    "    if len(solution) < 2:\n",
    "        return 0  # there is no travel\n",
    "    \n",
    "    total_dist = 0\n",
    "    for i in range(len(solution) - 1):\n",
    "        total_dist += W[solution[i], solution[i+1]].item()\n",
    "        \n",
    "    # if this solution is \"complete\", go back to initial point\n",
    "    if len(solution) == W.shape[0]:\n",
    "        total_dist += W[solution[-1], solution[0]].item()\n",
    "\n",
    "    return total_dist\n",
    "        \n",
    "def is_state_final(state):\n",
    "    return len(set(state.partial_solution)) == state.W.shape[0]\n",
    "\n",
    "def get_next_neighbor_random(state):\n",
    "    solution, W = state.partial_solution, state.W\n",
    "    \n",
    "    if len(solution) == 0:\n",
    "        return random.choice(range(W.shape[0]))\n",
    "    already_in = set(solution)\n",
    "    candidates = list(filter(lambda n: n.item() not in already_in, W[solution[-1]].nonzero()))\n",
    "    if len(candidates) == 0:\n",
    "        return None\n",
    "    return random.choice(candidates).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "23723fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1  # A seed for the random number generator\n",
    "\n",
    "# Graph\n",
    "NR_NODES = 10  # Number of nodes N\n",
    "EMBEDDING_DIMENSIONS = 5  # Embedding dimension D\n",
    "EMBEDDING_ITERATIONS_T = 1  # Number of embedding iterations T\n",
    "\n",
    "# Learning\n",
    "NR_EPISODES = 4001\n",
    "MEMORY_CAPACITY = 10000\n",
    "N_STEP_QL = 2  # Number of steps (n) in n-step Q-learning to wait before computing target reward estimate\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "GAMMA = 0.9\n",
    "INIT_LR = 5e-3\n",
    "LR_DECAY_RATE = 1. - 2e-5  # learning rate decay\n",
    "\n",
    "MIN_EPSILON = 0.1\n",
    "EPSILON_DECAY_RATE = 6e-4  # epsilon decay\n",
    "\n",
    "FOLDER_NAME = './models'  # where to checkpoint the best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b30c8b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(fname=None):\n",
    "    \"\"\" Create a new model. If fname is defined, load the model from the specified file.\n",
    "    \"\"\"\n",
    "    Q_net = QNet(EMBEDDING_DIMENSIONS, T=EMBEDDING_ITERATIONS_T).to(device)\n",
    "    optimizer = optim.Adam(Q_net.parameters(), lr=INIT_LR)\n",
    "    lr_scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=LR_DECAY_RATE)\n",
    "    \n",
    "    if fname is not None:\n",
    "        checkpoint = torch.load(fname)\n",
    "        Q_net.load_state_dict(checkpoint['model'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "    \n",
    "    Q_func = QFunction(Q_net, optimizer, lr_scheduler)\n",
    "    return Q_func, Q_net, optimizer, lr_scheduler\n",
    "\n",
    "def checkpoint_model(model, optimizer, lr_scheduler, loss, \n",
    "                     episode, avg_length):\n",
    "    if not os.path.exists(FOLDER_NAME):\n",
    "        os.makedirs(FOLDER_NAME)\n",
    "    \n",
    "    fname = os.path.join(FOLDER_NAME, 'ep_{}'.format(episode))\n",
    "    fname += '_length_{}'.format(avg_length)\n",
    "    fname += '.tar'\n",
    "    \n",
    "    torch.save({\n",
    "        'episode': episode,\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'lr_scheduler': lr_scheduler.state_dict(),\n",
    "        'loss': loss,\n",
    "        'avg_length': avg_length\n",
    "    }, fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "dfcb3835",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "can only convert an array of size 1 to a Python scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8112/1535062490.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[1;31m# explore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m             \u001b[0mnext_node\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_next_neighbor_random\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m             \u001b[0mnr_explores\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8112/836492327.py\u001b[0m in \u001b[0;36mget_next_neighbor_random\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0malready_in\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msolution\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mcandidates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0malready_in\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msolution\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8112/836492327.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(n)\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0malready_in\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msolution\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mcandidates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0malready_in\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msolution\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: can only convert an array of size 1 to a Python scalar"
     ]
    }
   ],
   "source": [
    "# seed everything for reproducible results first:\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Create module, optimizer, LR scheduler, and Q-function\n",
    "Q_func, Q_net, optimizer, lr_scheduler = init_model()\n",
    "\n",
    "# Create memory\n",
    "memory = Memory(MEMORY_CAPACITY)\n",
    "\n",
    "# Storing metrics about training:\n",
    "found_solutions = dict()  # episode --> (coords, W, solution)\n",
    "losses = []\n",
    "path_lengths = []\n",
    "\n",
    "# keep track of median path length for model checkpointing\n",
    "current_min_med_length = float('inf')\n",
    "\n",
    "for episode in range(NR_EPISODES):\n",
    "    # sample a new random graph\n",
    "    coords, W_np = coords, sl_mat\n",
    "    W = torch.tensor(W_np, dtype=torch.float32, requires_grad=False, device=device)\n",
    "    \n",
    "    # current partial solution - a list of node index\n",
    "    solution = [random.randint(0, NR_NODES-1)]\n",
    "    \n",
    "    # current state (tuple and tensor)\n",
    "    current_state = State(partial_solution=solution, W=sl_mat, coords=coords)\n",
    "    current_state_tsr = state2tens(current_state)\n",
    "    \n",
    "    # Keep track of some variables for insertion in replay memory:\n",
    "    states = [current_state]\n",
    "    states_tsrs = [current_state_tsr]  # we also keep the state tensors here (for efficiency)\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    \n",
    "    # current value of epsilon\n",
    "    epsilon = max(MIN_EPSILON, (1-EPSILON_DECAY_RATE)**episode)\n",
    "    \n",
    "    nr_explores = 0\n",
    "    t = -1\n",
    "    while not is_state_final(current_state):\n",
    "        t += 1  # time step of this episode\n",
    "        \n",
    "        if epsilon >= random.random():\n",
    "            # explore\n",
    "            next_node = get_next_neighbor_random(current_state)\n",
    "            nr_explores += 1\n",
    "        else:\n",
    "            # exploit\n",
    "            next_node, est_reward = Q_func.get_best_action(current_state_tsr, current_state)\n",
    "            if episode % 50 == 0:\n",
    "                print('Ep {} | current sol: {} / next est reward: {}'.format(episode, solution, est_reward))\n",
    "        \n",
    "        next_solution = solution + [next_node]\n",
    "        \n",
    "        # reward observed for taking this step        \n",
    "        reward = -(total_distance(next_solution, W) - total_distance(solution, W))\n",
    "        \n",
    "        next_state = State(partial_solution=next_solution, W=W, coords=coords)\n",
    "        next_state_tsr = state2tens(next_state)\n",
    "        \n",
    "        # store rewards and states obtained along this episode:\n",
    "        states.append(next_state)\n",
    "        states_tsrs.append(next_state_tsr)\n",
    "        rewards.append(reward)\n",
    "        actions.append(next_node)\n",
    "        \n",
    "        # store our experience in memory, using n-step Q-learning:\n",
    "        if len(solution) >= N_STEP_QL:\n",
    "            memory.remember(Experience(state=states[-N_STEP_QL],\n",
    "                                       state_tsr=states_tsrs[-N_STEP_QL],\n",
    "                                       action=actions[-N_STEP_QL],\n",
    "                                       reward=sum(rewards[-N_STEP_QL:]),\n",
    "                                       next_state=next_state,\n",
    "                                       next_state_tsr=next_state_tsr))\n",
    "            \n",
    "        if is_state_final(next_state):\n",
    "            for n in range(1, N_STEP_QL):\n",
    "                memory.remember(Experience(state=states[-n],\n",
    "                                           state_tsr=states_tsrs[-n], \n",
    "                                           action=actions[-n], \n",
    "                                           reward=sum(rewards[-n:]), \n",
    "                                           next_state=next_state,\n",
    "                                           next_state_tsr=next_state_tsr))\n",
    "        \n",
    "        # update state and current solution\n",
    "        current_state = next_state\n",
    "        current_state_tsr = next_state_tsr\n",
    "        solution = next_solution\n",
    "        \n",
    "        # take a gradient step\n",
    "        loss = None\n",
    "        if len(memory) >= BATCH_SIZE and len(memory) >= 2000:\n",
    "            experiences = memory.sample_batch(BATCH_SIZE)\n",
    "            \n",
    "            batch_states_tsrs = [e.state_tsr for e in experiences]\n",
    "            batch_Ws = [e.state.W for e in experiences]\n",
    "            batch_actions = [e.action for e in experiences]\n",
    "            batch_targets = []\n",
    "            \n",
    "            for i, experience in enumerate(experiences):\n",
    "                target = experience.reward\n",
    "                if not is_state_final(experience.next_state):\n",
    "                    _, best_reward = Q_func.get_best_action(experience.next_state_tsr, \n",
    "                                                            experience.next_state)\n",
    "                    target += GAMMA * best_reward\n",
    "                batch_targets.append(target)\n",
    "                \n",
    "            # print('batch targets: {}'.format(batch_targets))\n",
    "            loss = Q_func.batch_update(batch_states_tsrs, batch_Ws, batch_actions, batch_targets)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            \"\"\" Save model when we reach a new low average path length\n",
    "            \"\"\"\n",
    "            med_length = np.median(path_lengths[-100:])\n",
    "            if med_length < current_min_med_length:\n",
    "                current_min_med_length = med_length\n",
    "                checkpoint_model(Q_net, optimizer, lr_scheduler, loss, episode, med_length)\n",
    "                \n",
    "    length = total_distance(solution, W)\n",
    "    path_lengths.append(length)\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        print('Ep %d. Loss = %.3f / median length = %.3f / last = %.4f / epsilon = %.4f / lr = %.4f' % (\n",
    "            episode, (-1 if loss is None else loss), np.median(path_lengths[-50:]), length, epsilon,\n",
    "            Q_func.optimizer.param_groups[0]['lr']))\n",
    "        found_solutions[episode] = (W.clone(), coords.copy(), [n for n in solution])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e39ab29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
